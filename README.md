# nlp_word_encodings
This repository covers the concept of word encoding, which us a very important concept in the field of NLP It coveres various methodologies of word encoding, its evaluation, advantages/ disadvantages of different methodologies.

Please refer the following blog to understand the concepts in detail of the code present in this repository:

https://gyan-mittal.com/nlp-ai-ml/nlp-word-encoding/

NLP is a field of linguistics, computer science, and artificial intelligence. It helps computers understand, interpret and manipulate human language.

NLP is commonly used in speech recognition, natural language understanding, and natural language generation. It helps computers to understand natural language in order to perform some tasks.

Currently, Artificial Intelligence based deep learning is overtaken by others and is mostly used in NLP.

**High-level NLP use cases**
Some high-level Use cases that NLP addresses are as follows:

- Keyword Search
- Document Recommendations
- Speech recognition
- Classify text in predefined categories
- Clustering text in auto-generated categories
- Synonym finding
- Headline generation
- Summarization
- Translation
- Coreference (what ‘she’ or ‘he’ referring to in a document)
- Question Answering (Question answering based on the given text)
- Semantic Analysis (understanding the meaning of some statement)
- Name Entity Recognition (NER)
- Story Generation from key points
- Subtitle generation of video/ audio
- Generating Questions and Answers for a given text
- Chatbot
- Paraphrasing
- Generating story from data
- …And Much more


**Challenge**
In the field of NLP, generally, AI/ ML Algorithms don’t work on Text data. We have to find some way to represent text data into numerical data.

Any data consists of words. So in case, we find some way to convert (Encode) words into numerical data, then our whole data could be converted into numerical data, which can be consumed by AI/ ML algorithms.

We have the following ways of converting (Encoding) the words into numerical data. These encodings evolved over time and currently, Word Embeddings are the most preferred way to encode the words.

To get the details of various encodings are as following.

- Integer/ Label Encoding
- One-Hot Encoding
- Word Embeddings

After encoding the data, we can apply the various algorithms to solve the NLP use cases mentioned at the top of the document.

**Reference**
http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf